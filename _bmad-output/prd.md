---
stepsCompleted: [1, 2, 3, 4, 7, 8, 9, 10, 11]
inputDocuments:
  - '_bmad-output/project-planning-artifacts/product-brief-local-voice-assistant-claude-bmad-2025-12-26.md'
  - '_bmad-output/project-planning-artifacts/research/technical-voice-assistant-research-2025-12-26.md'
  - '_bmad-output/analysis/brainstorming-session-2025-12-26.md'
documentCounts:
  briefs: 1
  research: 1
  brainstorming: 1
  projectDocs: 0
workflowType: 'prd'
lastStep: 11
status: 'complete'
completedAt: '2025-12-27'
project_name: 'local-voice-assistant-claude-bmad'
user_name: 'y'
date: '2025-12-26'
---

# Product Requirements Document - local-voice-assistant-claude-bmad

**Author:** y
**Date:** 2025-12-26

## Executive Summary

**local-voice-assistant-claude-bmad** は、Private-first / On-device AI 思想に基づく日本語音声対話アシスタントです。

技術リテラシーのある個人ユーザー（エンジニア・研究者）を主要ターゲットとし、原則ローカル完結で動作しながら、必要に応じてクラウドLLMも選択できる柔軟なアーキテクチャを提供します。

### 解決する課題

- **プライバシーの懸念**: クラウドに音声データを送信することへの抵抗
- **コストの累積**: 従量課金APIの利用料金
- **日本語対応の限界**: 汎用モデルの日本語認識精度
- **ベンダーロックイン**: 特定サービスへの依存リスク

### What Makes This Special

1. **Private-first / On-device AI 思想**
   - 原則ローカル完結、必要に応じてクラウドLLM選択可能
   - 音声データがローカルに留まる安心感

2. **プラットフォーム性**
   - STT/LLM/TTSを差し替え可能なモジュラー設計
   - 研究用途（評価ログ・可視化）と実用用途の両立

3. **ChatGPTライクなUX**
   - 馴染みのあるチャットUI
   - ローカル会話履歴による継続的な対話体験

4. **日本語特化の高精度**
   - ReazonSpeech NeMo v2による Whisper超えの認識精度

## Project Classification

| 項目 | 値 |
|-----|---|
| **Technical Type** | Web App + API Backend (Hybrid) |
| **Domain** | Scientific (AI/ML, Research) |
| **Complexity** | Medium |
| **Project Context** | Greenfield - 新規プロジェクト |

**技術スタック:**
- Frontend: Next.js + TypeScript + @ricky0123/vad
- Backend: FastAPI + Pydantic + uv
- STT: ReazonSpeech NeMo v2
- LLM: OpenAI API互換 (Ollama/OpenAI/Groq)
- TTS: Style-BERT-VITS2 (GPU推奨)
- Communication: WebSocket
- Database: SQLite

## Success Criteria

### User Success

| 指標 | 成功基準 | 測定方法 |
|-----|---------|---------|
| **日常利用の定着** | 1週間連続で使い続ける | 使用ログ/自己観察 |
| **認識精度の体感** | 「ちゃんと聞き取れている」と感じる | STT出力の正確性確認 |
| **応答の自然さ** | 会話のリズムが途切れない | 体感レイテンシ |
| **セットアップ体験** | 「簡単だった」と感じる | 初回起動までの時間 |

**Aha! Moment:** 「クラウドに送らずにこの品質で動くんだ」と実感する瞬間

### Business Success

| タイムライン | 成功指標 |
|------------|---------|
| **3ヶ月後** | MVP完成、自分自身が毎日使用中 |
| **12ヶ月後** | v2.0完成（コンポーネント切り替え対応）、他の人にも使ってもらえている |

### Technical Success

| 指標 | 目標値 | 優先度 |
|-----|-------|-------|
| **STT認識精度** | > 95% (CER基準) | 高 |
| **応答遅延（E2E）** | < 2秒 | 高 |
| **セットアップ時間** | < 5分 (`uv run`で起動) | 中 |
| **評価ログ出力** | レイテンシ計測可能 | 中 |
| **コンポーネント切り替え** | 設定ファイルで完結 | 中（v2.0） |

### Measurable Outcomes

**研究用途の成功:**
- 評価ログ: JSON形式でSTT/LLM/TTSの各レイテンシを記録
- 比較実験: 設定ファイル変更のみでモデル切り替え可能
- 可視化: ログデータからグラフ生成可能な構造

**実用用途の成功:**
- 毎日の作業フローに自然に組み込める
- 起動→対話開始まで30秒以内
- 会話履歴が保持され、継続的な対話が可能

## Product Scope

### MVP - Minimum Viable Product

| 機能 | 実装内容 |
|-----|---------|
| **音声入力（STT）** | ReazonSpeech NeMo v2（ローカル固定） |
| **音声検出（VAD）** | @ricky0123/vad（フロントエンド） |
| **LLM応答** | OpenAI API互換（Ollama/OpenAI/Groq切り替え可能） |
| **音声出力（TTS）** | Style-BERT-VITS2（ローカル、GPU推奨） |
| **Web UI** | Next.js + TypeScript（ChatGPTライク） |
| **通信** | WebSocket（リアルタイム双方向） |
| **会話履歴** | SQLite（ローカル保存） |
| **評価ログ** | 各コンポーネントのレイテンシ記録 |

### Growth Features (Post-MVP) - v2.0

| 機能 | 説明 |
|-----|------|
| **STT切り替え** | ローカル（ReazonSpeech）⇔ クラウド（Whisper API/Google） |
| **TTS切り替え** | Style-BERT-VITS2 ⇔ VOICEVOX ⇔ Piper ⇔ クラウド |
| **設定UI** | Web画面からモデル切り替え |
| **複数会話管理** | 会話セッションの切り替え |

### Vision (Future) - v3.0+

| 機能 | 説明 |
|-----|------|
| **ウェイクワード検出** | 常時待機モード |
| **会話コンテキスト強化** | 長期記憶、要約機能 |
| **プラグインシステム** | 拡張機能の追加 |
| **Docker対応** | ワンクリックデプロイ |
| **ライブラリ化** | 他プロジェクトへの組み込み |

## User Journeys

### Journey 1: 田中 健太 - プライバシーを取り戻す開発者

田中健太は32歳のWebエンジニア。リモートワーク中心の生活で、自宅のデスクが彼の城だ。ChatGPTの音声機能は便利だと思っていたが、「自分の声がクラウドに送られている」という事実が、どうにも気になっていた。特に仕事の相談をする時、会社の機密情報が漏れないか心配だった。

ある週末、GitHubで「日本語 音声アシスタント ローカル」と検索していると、**local-voice-assistant-claude-bmad** を発見する。READMEを読むと「STT/TTSは完全ローカル処理」の文字。「これだ」と直感した。

`git clone` して `uv run` を実行。5分後、ブラウザでChatGPTライクな画面が開く。マイクボタンを押して「今日のタスクを整理したい」と話しかける。自分の声がテキストに変換され、LLMが応答し、Style-BERT-VITS2の声が返ってくる。**すべてがローカルで完結している。**

「これ、認識精度すごくない？」田中は驚く。技術的な専門用語も、早口で話しても、ほぼ完璧に認識される。Whisperより良いかもしれない。

1週間後、田中の朝のルーティンは変わっていた。コーヒーを淹れながら「今日のプルリクエスト、どこから手をつけるべき？」と話しかける。コードレビューの方針を相談する。リファクタリングのアイデアを壁打ちする。**声で考え、声で整理する。**キーボードを打つより自然に、思考が流れる。

プライバシーの心配は完全に消えた。会話履歴はローカルのSQLiteに保存され、いつでも見返せる。「これが本当のAIアシスタントだ」と田中は思った。

**この旅が明らかにする要件:**
- 5分以内のセットアップ（uv run）
- ChatGPTライクなWeb UI
- 高精度な日本語STT（技術用語、早口対応）
- ローカル会話履歴
- 自然な音声応答（Style-BERT-VITS2）

---

### Journey 2: 佐藤 美咲 - プロトタイプを3日で形にする

佐藤美咲は28歳、ヘルステックスタートアップのフルスタックエンジニア。チームから「次のデモまでに音声入力機能を追加できない？」と頼まれた。期限は2週間。

Google Cloud Speech-to-Textの料金ページを見て、美咲は頭を抱える。開発中のテストだけで数万円かかりそうだ。しかも、医療関連のアプリだから、音声データをクラウドに送るのは法務チェックが必要になる。「詰んだ...」

Slackで同僚に愚痴を言っていると、「ローカルで動く音声認識あるよ」とリンクが送られてくる。**local-voice-assistant-claude-bmad**。

美咲はまずデモを触る。「これ、日本語の認識精度高い...」。次にコードを読む。FastAPIのバックエンド、OpenAI API互換のLLMインターフェース。「うちのバックエンドにそのまま組み込めるじゃん！」

3日後、美咲はチームデモで音声入力機能を披露していた。ユーザーが症状を声で入力すると、テキスト化されて記録される。STTモジュールだけを切り出して、自社アプリに統合したのだ。

「APIコストゼロ、法務チェック不要、しかも精度も問題なし」と美咲は報告する。CTOが「これ、本番にも使えるね」と笑顔を見せた。

**この旅が明らかにする要件:**
- モジュラーなアーキテクチャ（STTだけ切り出し可能）
- OpenAI API互換インターフェース
- ドキュメント化されたAPI
- 商用利用可能なライセンス
- 低い導入障壁（依存関係の明確さ）

---

### Journey 3: 山田 教授 - 学生の研究を加速させる

山田教授は45歳、地方国立大学の情報工学科准教授。研究テーマは「日本語音声対話システムの評価手法」。問題は、学生が実験環境を構築するのに毎年2ヶ月かかることだ。

「先生、Whisperのセットアップでエラーが...」「CUDAのバージョンが合わなくて...」毎年同じ質問に答える日々。論文のためのコード実行に辿り着く前に、学生のモチベーションが下がってしまう。

ある学会で、山田教授は**local-voice-assistant-claude-bmad**の存在を知る。「uvで依存管理？セットアップ5分？」半信半疑でリポジトリをクローンする。

翌週のゼミ。「今日から使う実験環境を紹介します」と山田教授。学生たちがノートPCで`uv run`を実行すると、10分後には全員がブラウザで音声対話を試していた。「え、もう動くんですか？」学生の目が輝く。

そして山田教授が最も気に入ったのは、**評価ログ機能**だった。STT/LLM/TTSの各レイテンシがJSON形式で記録される。「これで比較実験のデータ収集が自動化できる」。設定ファイルでモデルを切り替えれば、異なる条件での比較も簡単だ。

学期末、学生たちは例年より2ヶ月早く実験結果を出していた。「やっと研究の本質に時間を使える」と山田教授は満足げだった。

**この旅が明らかにする要件:**
- 評価ログ出力（JSON形式）
- 各コンポーネントのレイテンシ計測
- 設定ファイルでのモデル切り替え
- 再現性のある環境（uv依存管理）
- 明確なドキュメント（学生向け）

---

### Journey Requirements Summary

| ユーザータイプ | 主要な要件 |
|--------------|-----------|
| **個人ユーザー（田中）** | 簡単セットアップ、ChatGPT風UI、ローカル会話履歴、高精度STT |
| **開発者（佐藤）** | モジュラー設計、API互換、ドキュメント、組み込み容易性 |
| **研究者（山田）** | 評価ログ、レイテンシ計測、設定切り替え、再現性 |

**共通要件:**
- 5分以内のセットアップ（uv run）
- 高精度な日本語STT（ReazonSpeech）
- ローカル完結のプライバシー保護
- WebSocket によるリアルタイム通信

## Web App + API Backend 固有要件

### プロジェクトタイプ概要

本プロジェクトはハイブリッド構成で、**Next.js フロントエンド**と **FastAPI バックエンド**がWebSocketでリアルタイム通信を行います。

| 層 | 技術 | 役割 |
|----|------|------|
| Frontend | Next.js + TypeScript | ChatGPTライクなUI、VAD音声検出 |
| Backend | FastAPI + Pydantic | STT/LLM/TTS パイプライン処理 |
| Communication | WebSocket | 双方向リアルタイム通信 |
| Storage | SQLite | 会話履歴・設定保存 |

### ブラウザ対応

| ブラウザ | 対応 | 備考 |
|---------|------|------|
| Chrome (最新) | ✅ | 主要開発環境 |
| Firefox (最新) | ✅ | |
| Safari (最新) | ✅ | WebAudio API対応確認必要 |
| Edge (最新) | ✅ | Chromiumベース |
| IE / 旧ブラウザ | ❌ | 対象外 |

**Web Audio API要件**: VAD（@ricky0123/vad）がマイク入力を使用するため、HTTPS環境またはlocalhostが必要。

### API設計方針

#### エンドポイント構成

| エンドポイント | メソッド | 説明 |
|--------------|---------|------|
| `/ws/chat` | WebSocket | リアルタイム音声対話 |
| `/api/v1/conversations` | GET/POST | 会話履歴管理 |
| `/api/v1/config` | GET/PUT | 設定管理 |
| `/api/v1/health` | GET | ヘルスチェック |

#### バージョニング戦略

- **URL Path方式**: `/api/v1/...`
- 将来のライブラリ化を見据えた設計
- 破壊的変更時は新バージョン (`/api/v2/`) を追加

#### 認証

- **MVP**: 認証不要（ローカル個人利用）
- **将来**: 必要に応じてJWT/API Key対応を検討

### レート制限

| 対象 | 制限 | 理由 |
|-----|------|------|
| LLM API呼び出し | 設定可能な上限 | クラウドLLM利用時のコスト管理 |
| WebSocket接続 | 単一接続推奨 | リソース効率 |

設定ファイルで制限値を調整可能とする。

### パフォーマンス目標

| 指標 | 目標 | 測定方法 |
|-----|------|---------|
| 初回ロード | < 3秒 | Lighthouse |
| WebSocket接続確立 | < 500ms | 計測ログ |
| E2E応答遅延 | < 2秒 | STT開始→TTS再生開始 |

### 実装上の考慮事項

1. **CORS設定**: 開発環境でのlocalhost許可
2. **WebSocket再接続**: 接続断時の自動リトライ
3. **音声データ形式**: WAV/PCM（STT入力）、WAV（TTS出力）
4. **エラーハンドリング**: WebSocketメッセージでエラー通知

## リスク分析と軽減策

### MVP戦略

**アプローチ:** Problem-Solving MVP + Platform MVP ハイブリッド

- **Problem-Solving**: プライバシー・コスト・日本語精度の課題を直接解決
- **Platform**: 将来のSTT/TTS切り替えを見据えたモジュラー設計

### リスクマトリクス

| リスク種別 | リスク内容 | 影響度 | 発生確率 | 軽減策 |
|-----------|-----------|--------|---------|--------|
| **技術** | ReazonSpeech NeMo v2のGPUメモリ要件（~4GB VRAM） | 高 | 中 | CPU推論フォールバック、モデル量子化検討 |
| **技術** | WebSocket接続の不安定性 | 中 | 中 | 自動再接続、状態復元、ハートビート |
| **技術** | Style-BERT-VITS2のGPU要件 | 中 | 中 | GPU推奨、CPU動作可（低速） |
| **市場** | ターゲットユーザーが限定的 | 低 | 低 | 研究用途と実用の両立でリーチ拡大 |
| **リソース** | 個人開発の継続性 | 中 | 中 | MVPをシンプルに保ち完成優先 |

### リソース要件

**MVP開発:**
- 開発者: 1名（個人プロジェクト）
- 開発環境: NVIDIA GPU搭載PC（推奨）またはCPU環境
- 外部依存: Ollama（ローカルLLM使用時）
- 備考: Style-BERT-VITS2はインプロセス統合（別途起動不要）、GPU推奨

## Functional Requirements

### 音声入力（STT）

- **FR1**: ユーザーはマイクを通じて日本語音声を入力できる
- **FR2**: システムはVADにより発話の開始と終了を自動検出できる
- **FR3**: システムは音声をリアルタイムでテキストに変換できる
- **FR4**: ユーザーは認識されたテキストを画面上で確認できる

### LLM対話

- **FR5**: ユーザーはテキスト化された発話に対してLLMからの応答を受け取れる
- **FR6**: システムは会話コンテキストを維持して応答を生成できる
- **FR7**: システムはOpenAI API互換インターフェースを通じて複数のLLMプロバイダーに接続できる
- **FR8**: ユーザーはLLMプロバイダー（Ollama/OpenAI/Groq）を設定で切り替えられる

### 音声出力（TTS）

- **FR9**: システムはLLM応答をStyle-BERT-VITS2を使用して音声に変換できる
- **FR10**: ユーザーは音声応答をスピーカーで聴くことができる
- **FR11**: ユーザーは音声出力中にテキストも同時に確認できる

### Web UI

- **FR12**: ユーザーはブラウザでチャット形式のUIにアクセスできる
- **FR13**: ユーザーはマイクボタンで録音を開始/停止できる
- **FR14**: ユーザーは会話履歴をスクロールして閲覧できる
- **FR15**: ユーザーは現在の録音状態（待機/録音中/処理中）を視覚的に確認できる

### 会話履歴管理

- **FR16**: システムは会話履歴をローカルデータベースに保存できる
- **FR17**: ユーザーは過去の会話履歴を閲覧できる
- **FR18**: ユーザーは会話履歴を削除できる

### 設定管理

- **FR19**: ユーザーはLLMプロバイダーの接続設定を構成できる
- **FR20**: ユーザーはLLM APIのレート制限を設定できる
- **FR21**: システムは設定をローカルファイルで永続化できる

### 評価・ログ機能

- **FR22**: システムはSTT処理のレイテンシを記録できる
- **FR23**: システムはLLM応答のレイテンシを記録できる
- **FR24**: システムはTTS処理のレイテンシを記録できる
- **FR25**: システムは評価ログをJSON形式で出力できる
- **FR26**: ユーザーはログデータを分析用にエクスポートできる

### リアルタイム通信

- **FR27**: システムはWebSocketを通じてフロントエンドとバックエンド間でリアルタイム通信できる
- **FR28**: システムは接続断時に自動再接続を試行できる
- **FR29**: システムはエラー発生時にユーザーに通知できる

### システム起動・ヘルスチェック

- **FR30**: ユーザーは`uv run`コマンドでシステムを起動できる
- **FR31**: システムはヘルスチェックAPIを提供できる
- **FR32**: システムはバックエンドサービスの状態を報告できる

## Non-Functional Requirements

### Performance

| NFR ID | 要件 | 測定基準 |
|--------|------|---------|
| **NFR-P1** | 音声認識（STT）は発話終了から2秒以内に完了する | STTレイテンシ < 2000ms |
| **NFR-P2** | LLM応答は最初のトークンが1秒以内に到着する | Time to First Token < 1000ms |
| **NFR-P3** | TTS音声生成は1秒以内に開始する | TTS開始レイテンシ < 1000ms |
| **NFR-P4** | E2E応答遅延は2秒以内に収まる | 発話終了→TTS再生開始 < 2000ms |
| **NFR-P5** | Web UIの初回ロードは3秒以内に完了する | Lighthouse Performance Score > 80 |
| **NFR-P6** | WebSocket接続確立は500ms以内に完了する | 接続レイテンシ < 500ms |

### Integration

| NFR ID | 要件 | 測定基準 |
|--------|------|---------|
| **NFR-I1** | OpenAI API互換インターフェースを提供する | OpenAI SDK互換のリクエスト/レスポンス形式 |
| **NFR-I2** | Style-BERT-VITS2をインプロセスで呼び出せる | Python直接呼び出し |
| **NFR-I3** | 設定ファイルはYAML/JSON形式で管理する | 人間可読な設定フォーマット |

### Reliability

| NFR ID | 要件 | 測定基準 |
|--------|------|---------|
| **NFR-R1** | WebSocket切断時に自動再接続を試行する | 最大3回のリトライ、指数バックオフ |
| **NFR-R2** | バックエンドサービス起動時にヘルスチェックを実施する | `/api/v1/health` で依存サービス状態確認 |
| **NFR-R3** | エラー発生時にユーザーに分かりやすいメッセージを表示する | エラーメッセージの日本語対応 |
| **NFR-R4** | 会話履歴はローカルDBに永続化され、再起動後も保持される | SQLite永続化 |

### Maintainability

| NFR ID | 要件 | 測定基準 |
|--------|------|---------|
| **NFR-M1** | コードはRuff/ESLintでリント検証に合格する | CI/ローカルでリントエラーゼロ |
| **NFR-M2** | 評価ログはJSON形式で構造化される | 各コンポーネントのレイテンシ記録 |
| **NFR-M3** | 依存関係はuv/npm lockfileで固定される | 再現可能なビルド環境 |

